{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a spam classificator (a more challenging exercise). Steps:**\n",
    "- **[X] Download spam examples and standart e-mails of public datasets from Apache SpamAssassin (https://spamassassin.apache.org/old/publiccorpus/);**\n",
    "- **[X] Unzip the datasets and try to get familiarized with the data format;**\n",
    "- **[X] Split the datasets in a training set and a test set**\n",
    "- **[X] Write a data preparation pipeline to convert each e-mail in a vector of characteristics. Your preparation pipeline should transform an e-mail to a vector (sparse) that indicates the presence or not of each possible word. For example, if all e-mails have only four words, 'Hello', 'how', 'are', 'you', then the e-mail 'Hello you Hello Hello you' would be converted to a vector [1, 0, 0, 1] (meaning that 'Hello' is present, 'how' is absent, 'are' is absent and 'you' is present), or [3, 0, 0, 2] if you prefer to count the number of occurences of each word;**\n",
    "- **[X] Maybe you want to add hyperparameters to your preparation pipeline to control wether or not to remove the headers of e-mails, convert each e-mail to lowercase, remove ponctuation, replace all URLs to \"URL\", replace all numbers to \"NUMBER\", or even reduce, that is, remove word endings. There are libraries in Python availble to do that.**\n",
    "\n",
    "- **[X] Following, try Logistic Regressor and see if you can build a good spam classifier with high revocation and precision.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = 'https://spamassassin.apache.org/old/publiccorpus/'\n",
    "SPAM_FILE = '20021010_spam.tar.bz2'\n",
    "HAM_FILE = '20021010_easy_ham.tar.bz2'\n",
    "SPAM_URL = DOWNLOAD_ROOT + SPAM_FILE\n",
    "HAM_URL = DOWNLOAD_ROOT + HAM_FILE\n",
    "PATH = os.path.join(\"datasets\", \"spam\")\n",
    "\n",
    "def fetch_data(spam_url=SPAM_URL, path=PATH):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "    for filename, url in (('ham.tar.bz2', HAM_URL), ('spam.tar.bz2', SPAM_URL)):\n",
    "        path = os.path.join(PATH, filename)\n",
    "        if not os.path.isfile(path):\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "        tar_bz2_file = tarfile.open(path)\n",
    "        tar_bz2_file.extractall(path=PATH)\n",
    "        tar_bz2_file.close()\n",
    "\n",
    "fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From exmh-workers-admin@redhat.com  Thu Aug 22 12:36:23 2002\n",
      "Return-Path: <exmh-workers-admin@example.com>\n",
      "Delivered-To: zzzz@localhost.netnoteinc.com\n",
      "Received: from localhost (localhost [127.0.0.1])\n",
      "\tby phobos.labs.netnoteinc.com (Postfix) with ESMTP id D03E543C36\n",
      "\tfor <zzzz@localhost>; Thu, 22 Aug 2002 07:36:16 -0400 (EDT)\n",
      "Received: from phobos [127.0.0.1]\n",
      "\tby localhost with IMAP (fetchmail-5.9.0)\n",
      "\tfor zzzz@localhost (single-drop); Thu, 22 Aug 2002 12:36:16 +0100 (IST)\n",
      "Received: from listman.e [...]\n"
     ]
    }
   ],
   "source": [
    "# Take a look in the files\n",
    "file = open('./datasets/spam/easy_ham/0001.ea7e79d3153e7469e7a9c3e0af6a357e', 'r')\n",
    "print(file.read()[:500], '[...]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split e-mails\n",
    "import email\n",
    "import email.policy\n",
    "\n",
    "HAM_DIR = os.path.join(PATH, \"easy_ham\")\n",
    "SPAM_DIR = os.path.join(PATH, \"spam\")\n",
    "ham_filenames = [i for i in os.listdir(HAM_DIR)]\n",
    "spam_filenames = [i for i in os.listdir(SPAM_DIR)]\n",
    "\n",
    "def load_email(is_spam, filename, spam_path=PATH):\n",
    "    directory = \"spam\" if is_spam else \"easy_ham\"\n",
    "    f = open(os.path.join(spam_path, directory, filename), \"rb\")\n",
    "    return email.parser.BytesParser(policy=email.policy.default).parse(f)\n",
    "\n",
    "ham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames]\n",
    "spam_emails = [load_email(is_spam=True, filename=name) for name in spam_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ham files: 2551 . Spam files: 501\n"
     ]
    }
   ],
   "source": [
    "print('Ham files:', len(ham_emails),'. Spam files:', len(spam_emails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete multipart\n",
    "ham_emails = [i for i in ham_emails if i.is_multipart()==False]\n",
    "spam_emails = [i for i in spam_emails if i.is_multipart()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([i for i in (ham_emails+spam_emails)])\n",
    "y = np.concatenate((np.ones(len(ham_emails)), np.zeros(len(spam_emails))))\n",
    "test_size = len(spam_emails)/len(ham_emails)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class emailToString():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    # Return text from e-mail\n",
    "    def to_text(self, text):\n",
    "        return str(text.get_payload())\n",
    "    \n",
    "    # Transform to lowercase, replace urls to 'URL', replace numbers to 'NUMBER'\n",
    "    # Remove '\\n', remove punctuation\n",
    "    def text_transform(self, t):\n",
    "        t = t.lower()\n",
    "        t = re.sub(r'http\\S+', 'URL', t)\n",
    "        t = re.sub(r'www\\S+', 'URL', t)\n",
    "        t = re.sub(r'\\d\\S+', 'NUMBER', t)\n",
    "        t = re.sub(r'\\n', ' ', t)\n",
    "        t = t.translate(str.maketrans(' ', ' ', string.punctuation))\n",
    "        return t\n",
    "    \n",
    "    # Return array of e-mails texts\n",
    "    def fit(self, emails, y=None):\n",
    "        X_train_fitted = []\n",
    "        for email in emails:\n",
    "            text = self.to_text(email)\n",
    "            text = self.text_transform(text)\n",
    "            X_train_fitted.append(text)\n",
    "        X_train_fitted = np.array(X_train_fitted)\n",
    "        return X_train_fitted\n",
    "    \n",
    "    # Transform email in vector\n",
    "    def to_vector(self, text):\n",
    "        text=[text]\n",
    "        vectrans = CountVectorizer()\n",
    "        vectrans.fit(text)\n",
    "        vector = vectrans.transform(text)\n",
    "        return vector.toarray()[0].astype('int32')\n",
    "        \n",
    "    # Return list of vector of each email\n",
    "    def transform(self, emails):\n",
    "        X_train_transformed = []\n",
    "        for email in emails:\n",
    "            vector = self.to_vector(email)\n",
    "            X_train_transformed.append(vector)\n",
    "        X_train_transformed = np.array(X_train_transformed)\n",
    "        return X_train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# This could raise an error!\n",
    "print(len(emailToString().transform(emailToString().fit(X_train))) == len(X_train))\n",
    "print(len(emailToString().transform(emailToString().fit(X_train))) == len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_strings = emailToString().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocabulary(all_strings):\n",
    "    vocabulary = []\n",
    "    for i in range(len(all_strings)):\n",
    "        words_in_string = all_strings[i].split()\n",
    "        for word in words_in_string:\n",
    "            if word not in vocabulary:\n",
    "                vocabulary.append(word)\n",
    "    return vocabulary\n",
    "\n",
    "vocabulary = make_vocabulary(all_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_vector(all_strings):\n",
    "    X_all = []\n",
    "    for email in all_strings:\n",
    "        words_in_email = []\n",
    "        for word in vocabulary:\n",
    "            words_in_email.append(email.count(word))\n",
    "        X_all.append(words_in_email)\n",
    "    return np.array(X_all)\n",
    "\n",
    "vectors = create_vector(all_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................................... , score=0.984, total=   3.0s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    3.0s remaining:    0.0s\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................................... , score=0.986, total=   2.9s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    5.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................................... , score=0.985, total=   2.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    8.3s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9849125080328266"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_reg = LogisticRegression(solver=\"liblinear\", random_state=42)\n",
    "score = cross_val_score(log_reg, vectors, y_train, cv=3, verbose=3)\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora quero transformar X_test, de forma q eu só precise jogar o X_test_trans no log_reg pra funcionar o predict\n",
    "# Pra isso, é bom separar oq cada função faz, e montar um pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "preprop = Pipeline([\n",
    "    (\"transformer\", emailToString()),\n",
    "])\n",
    "\n",
    "#X_trans = preprop.fit(X_train)\n",
    "#X_transformed = preprop.transform(X_trans)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a spam classificator (a more challenging exercise):**\n",
    "- **[X] Download spam examples and standart e-mails of public datasets from Apache SpamAssassin (https://spamassassin.apache.org/old/publiccorpus/);**\n",
    "- **[X] Unzip the datasets and try to get familiarized with the data format;**\n",
    "- **[X] Split the datasets in a training set and a test set**\n",
    "- **[ ] Write a data preparation pipeline to convert each e-mail in a vector of characteristics. Your preparation pipeline should transform an e-mail to a vector (sparse) that indicates the presence or not of each possible word. For example, if all e-mails have only four words, 'Hello', 'how', 'are', 'you', then the e-mail 'Hello you Hello Hello you' would be converted to a vector [1, 0, 0, 1] (meaning that 'Hello' is present, 'how' is absent, 'are' is absent and 'you' is present), or [3, 0, 0, 2] if you prefer to count the number of occurences of each word;**\n",
    "- **[X] Maybe you want to add hyperparameters to your preparation pipeline to control wether or not to remove the headers of e-mails, convert each e-mail to lowercase, remove ponctuation, replace all URLs to \"URL\", replace all numbers to \"NUMBER\", or even reduce, that is, remove word endings. There are libraries in Python availble to do that.**\n",
    "\n",
    "- **[ ] Following, try many classifiers and see if you can build a good spam classifier with high revocation and precision.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I will try to get more than 97% recall**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = 'https://spamassassin.apache.org/old/publiccorpus/'\n",
    "SPAM_FILE = '20021010_spam.tar.bz2'\n",
    "HAM_FILE = '20021010_easy_ham.tar.bz2'\n",
    "SPAM_URL = DOWNLOAD_ROOT + SPAM_FILE\n",
    "HAM_URL = DOWNLOAD_ROOT + HAM_FILE\n",
    "PATH = os.path.join(\"datasets\", \"spam\")\n",
    "\n",
    "def fetch_data(spam_url=SPAM_URL, path=PATH):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "    for filename, url in (('ham.tar.bz2', HAM_URL), ('spam.tar.bz2', SPAM_URL)):\n",
    "        path = os.path.join(PATH, filename)\n",
    "        if not os.path.isfile(path):\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "        tar_bz2_file = tarfile.open(path)\n",
    "        tar_bz2_file.extractall(path=PATH)\n",
    "        tar_bz2_file.close()\n",
    "\n",
    "fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From exmh-workers-admin@redhat.com  Thu Aug 22 12:36:23 2002\n",
      "Return-Path: <exmh-workers-admin@example.com>\n",
      "Delivered-To: zzzz@localhost.netnoteinc.com\n",
      "Received: from localhost (localhost [127.0.0.1])\n",
      "\tby phobos.labs.netnoteinc.com (Postfix) with ESMTP id D03E543C36\n",
      "\tfor <zzzz@localhost>; Thu, 22 Aug 2002 07:36:16 -0400 (EDT)\n",
      "Received: from phobos [127.0.0.1]\n",
      "\tby localhost with IMAP (fetchmail-5.9.0)\n",
      "\tfor zzzz@localhost (single-drop); Thu, 22 Aug 2002 12:36:16 +0100 (IST)\n",
      "Received: from listman.e [...]\n"
     ]
    }
   ],
   "source": [
    "# Take a look in the files\n",
    "file = open('./datasets/spam/easy_ham/0001.ea7e79d3153e7469e7a9c3e0af6a357e', 'r')\n",
    "print(file.read()[:500], '[...]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split e-mails\n",
    "import email\n",
    "import email.policy\n",
    "\n",
    "HAM_DIR = os.path.join(PATH, \"easy_ham\")\n",
    "SPAM_DIR = os.path.join(PATH, \"spam\")\n",
    "ham_filenames = [i for i in os.listdir(HAM_DIR)]\n",
    "spam_filenames = [i for i in os.listdir(SPAM_DIR)]\n",
    "\n",
    "def load_email(is_spam, filename, spam_path=PATH):\n",
    "    directory = \"spam\" if is_spam else \"easy_ham\"\n",
    "    f = open(os.path.join(spam_path, directory, filename), \"rb\")\n",
    "    return email.parser.BytesParser(policy=email.policy.default).parse(f)\n",
    "\n",
    "ham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames]\n",
    "spam_emails = [load_email(is_spam=True, filename=name) for name in spam_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ham files: 2551 . Spam files: 501\n"
     ]
    }
   ],
   "source": [
    "print('Ham files:', len(ham_emails),'. Spam files:', len(spam_emails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete multipart\n",
    "ham_emails = [i for i in ham_emails if i.is_multipart()==False]\n",
    "spam_emails = [i for i in spam_emails if i.is_multipart()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([i for i in (ham_emails+spam_emails)])\n",
    "y = np.concatenate((np.ones(len(ham_emails)), np.zeros(len(spam_emails))))\n",
    "test_size = len(spam_emails)/len(ham_emails)\n",
    "\n",
    "X_train, y_train, X_test, y_test = train_test_split(X, y, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return text from e-mail\n",
    "def to_text(text):\n",
    "    return str(text.get_payload())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to lowercase\n",
    "# Replace urls to 'URL'\n",
    "# Replace numbers to 'NUMBER'\n",
    "# Remove '\\n'\n",
    "# Remove punctuation\n",
    "import re\n",
    "import string\n",
    "\n",
    "def text_transform(t):\n",
    "    t = t.lower()\n",
    "    t = re.sub(r'http\\S+', 'URL', t)\n",
    "    t = re.sub(r'www\\S+', 'URL', t)\n",
    "    t = re.sub(r'\\d\\S+', 'NUMBER', t)\n",
    "    t = re.sub(r'\\n', ' ', t)\n",
    "    t = t.translate(str.maketrans(' ', ' ', string.punctuation))\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sparse vector\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def to_vector(text):\n",
    "    text=[text]\n",
    "    vectrans = CountVectorizer()\n",
    "    vectrans.fit(text)\n",
    "    vector = vectrans.transform(text)\n",
    "    return vector.toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from email import parser\n",
    "\n",
    "class emailToString(BaseEstimator, TransformerMixin, parser.BytesParser):\n",
    "    def __init__(self):\n",
    "        self.X = None\n",
    "    \n",
    "    def fit(self, emails):\n",
    "        X_train_fitted = []\n",
    "        for email in emails:\n",
    "            text = to_text(email)\n",
    "            text = text_transform(text)\n",
    "            X_train_fitted.append(text)\n",
    "        X_train_fitted = np.array(X_train_fitted)\n",
    "        return X_train_fitted\n",
    "        \n",
    "    def transform(self, emails):\n",
    "        X_train_transformed = []\n",
    "        for email in emails:\n",
    "            vector = to_vector(email)\n",
    "            X_train_transformed.append(vector)\n",
    "        X_train_transformed = np.array(X_train_transformed)\n",
    "        return X_train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('text_transform', emailToString()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-02e54ed9f2a9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_train_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    333\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'passthrough'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: fit() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "X_train_new = pipeline.fit(X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

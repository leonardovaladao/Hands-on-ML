{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Build a spam classificator (a more challenging exercise). Steps:**\n",
    "- **[X] Download spam examples and standart e-mails of public datasets from Apache SpamAssassin (https://spamassassin.apache.org/old/publiccorpus/);**\n",
    "- **[X] Unzip the datasets and try to get familiarized with the data format;**\n",
    "- **[X] Split the datasets in a training set and a test set**\n",
    "- **[X] Write a data preparation pipeline to convert each e-mail in a vector of characteristics. Your preparation pipeline should transform an e-mail to a vector (sparse) that indicates the presence or not of each possible word. For example, if all e-mails have only four words, 'Hello', 'how', 'are', 'you', then the e-mail 'Hello you Hello Hello you' would be converted to a vector [1, 0, 0, 1] (meaning that 'Hello' is present, 'how' is absent, 'are' is absent and 'you' is present), or [3, 0, 0, 2] if you prefer to count the number of occurences of each word;**\n",
    "- **[X] Maybe you want to add hyperparameters to your preparation pipeline to control wether or not to remove the headers of e-mails, convert each e-mail to lowercase, remove ponctuation, replace all URLs to \"URL\", replace all numbers to \"NUMBER\", or even reduce, that is, remove word endings. There are libraries in Python availble to do that.**\n",
    "\n",
    "- **[X] Following, try Logistic Regressor and see if you can build a good spam classifier with high revocation and precision.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I will try to get more than 97% recall**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = 'https://spamassassin.apache.org/old/publiccorpus/'\n",
    "SPAM_FILE = '20021010_spam.tar.bz2'\n",
    "HAM_FILE = '20021010_easy_ham.tar.bz2'\n",
    "SPAM_URL = DOWNLOAD_ROOT + SPAM_FILE\n",
    "HAM_URL = DOWNLOAD_ROOT + HAM_FILE\n",
    "PATH = os.path.join(\"datasets\", \"spam\")\n",
    "\n",
    "def fetch_data(spam_url=SPAM_URL, path=PATH):\n",
    "    if not os.path.isdir(path):\n",
    "        os.makedirs(path)\n",
    "    for filename, url in (('ham.tar.bz2', HAM_URL), ('spam.tar.bz2', SPAM_URL)):\n",
    "        path = os.path.join(PATH, filename)\n",
    "        if not os.path.isfile(path):\n",
    "            urllib.request.urlretrieve(url, path)\n",
    "        tar_bz2_file = tarfile.open(path)\n",
    "        tar_bz2_file.extractall(path=PATH)\n",
    "        tar_bz2_file.close()\n",
    "\n",
    "fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From exmh-workers-admin@redhat.com  Thu Aug 22 12:36:23 2002\n",
      "Return-Path: <exmh-workers-admin@example.com>\n",
      "Delivered-To: zzzz@localhost.netnoteinc.com\n",
      "Received: from localhost (localhost [127.0.0.1])\n",
      "\tby phobos.labs.netnoteinc.com (Postfix) with ESMTP id D03E543C36\n",
      "\tfor <zzzz@localhost>; Thu, 22 Aug 2002 07:36:16 -0400 (EDT)\n",
      "Received: from phobos [127.0.0.1]\n",
      "\tby localhost with IMAP (fetchmail-5.9.0)\n",
      "\tfor zzzz@localhost (single-drop); Thu, 22 Aug 2002 12:36:16 +0100 (IST)\n",
      "Received: from listman.e [...]\n"
     ]
    }
   ],
   "source": [
    "# Take a look in the files\n",
    "file = open('./datasets/spam/easy_ham/0001.ea7e79d3153e7469e7a9c3e0af6a357e', 'r')\n",
    "print(file.read()[:500], '[...]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split e-mails\n",
    "import email\n",
    "import email.policy\n",
    "\n",
    "HAM_DIR = os.path.join(PATH, \"easy_ham\")\n",
    "SPAM_DIR = os.path.join(PATH, \"spam\")\n",
    "ham_filenames = [i for i in os.listdir(HAM_DIR)]\n",
    "spam_filenames = [i for i in os.listdir(SPAM_DIR)]\n",
    "\n",
    "def load_email(is_spam, filename, spam_path=PATH):\n",
    "    directory = \"spam\" if is_spam else \"easy_ham\"\n",
    "    f = open(os.path.join(spam_path, directory, filename), \"rb\")\n",
    "    return email.parser.BytesParser(policy=email.policy.default).parse(f)\n",
    "\n",
    "ham_emails = [load_email(is_spam=False, filename=name) for name in ham_filenames]\n",
    "spam_emails = [load_email(is_spam=True, filename=name) for name in spam_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ham files: 2551 . Spam files: 501\n"
     ]
    }
   ],
   "source": [
    "print('Ham files:', len(ham_emails),'. Spam files:', len(spam_emails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete multipart\n",
    "ham_emails = [i for i in ham_emails if i.is_multipart()==False]\n",
    "spam_emails = [i for i in spam_emails if i.is_multipart()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([i for i in (ham_emails+spam_emails)])\n",
    "y = np.concatenate((np.ones(len(ham_emails)), np.zeros(len(spam_emails))))\n",
    "test_size = len(spam_emails)/len(ham_emails)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class emailHandler():\n",
    "    def __init__(self, email_list):        \n",
    "        import email\n",
    "        \n",
    "        ty = type(email_list)\n",
    "        tyem = type(email.message.EmailMessage())\n",
    "        # If email_list is not a list or array...\n",
    "        if not (ty == np.ndarray or ty == list):\n",
    "            raise Exception('Object type not supported. Please pass a list or numpy array.')\n",
    "        # If objects in email_list are not emails...\n",
    "        elif not (type(email_list[0]) == tyem):\n",
    "            raise Exception('Please pass a list or array of email.message.EmailMessage objects.',\n",
    "                            'Use the email library to transform your objects.')\n",
    "        else:\n",
    "            # Define list of e-mails\n",
    "            self.email_list = email_list\n",
    "    \n",
    "    # Take a list of email objects and transform in a list of email texts\n",
    "    def create_email_list(self):\n",
    "        import re\n",
    "        import string\n",
    "        \n",
    "        # Return text from e-mail\n",
    "        def to_text(text):\n",
    "            return str(text.get_payload())\n",
    "\n",
    "        # Transform to lowercase, replace urls to 'URL', replace numbers to 'NUMBER'\n",
    "        # Remove '\\n', remove punctuation\n",
    "        def text_transform(t):\n",
    "            t = t.lower()\n",
    "            t = re.sub(r'http\\S+', 'URL', t)\n",
    "            t = re.sub(r'www\\S+', 'URL', t)\n",
    "            t = re.sub(r'\\d\\S+', 'NUMBER', t)\n",
    "            t = re.sub(r'\\n', ' ', t)\n",
    "            t = t.translate(str.maketrans(' ', ' ', string.punctuation))\n",
    "            return t\n",
    "\n",
    "        # Return array of e-mails texts\n",
    "        \n",
    "        X_train_fitted = []\n",
    "        for email in self.email_list:\n",
    "            text = to_text(email)\n",
    "            text = text_transform(text)\n",
    "            X_train_fitted.append(text)\n",
    "        return np.array(X_train_fitted)\n",
    "    \n",
    "    # Create word vocabulary\n",
    "    def make_vocabulary(self):\n",
    "        all_strings = self.create_email_list()\n",
    "        self.vocabulary = []\n",
    "        for i in range(len(all_strings)):\n",
    "            words_in_string = all_strings[i].split()\n",
    "            for word in words_in_string:\n",
    "                if word not in self.vocabulary:\n",
    "                    self.vocabulary.append(word)\n",
    "        return self.vocabulary\n",
    "    \n",
    "    # Transform each email in a vector, where each instance is the number of how many times the word\n",
    "    # appears in that email. The index for each word is stablished by the make_vocabulary() function\n",
    "    def create_vector(self, vocabulary):\n",
    "        all_strings = self.create_email_list()\n",
    "        X_all = []\n",
    "        for email in all_strings:\n",
    "            words_in_email = []\n",
    "            for word in vocabulary:\n",
    "                words_in_email.append(email.count(word))\n",
    "            X_all.append(words_in_email)\n",
    "        return np.array(X_all)\n",
    "    \n",
    "    def fit(self, X=0, y=0):\n",
    "        self.make_vocabulary()\n",
    "        pass\n",
    "    \n",
    "    def transform(self):\n",
    "        vectors = self.create_vector(self.vocabulary)\n",
    "        return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 1, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create list of vectors\n",
    "\n",
    "# eH has training set emails\n",
    "eH = emailHandler(X_train)\n",
    "eH.fit()\n",
    "X_train_transformed = eH.transform()\n",
    "X_train_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.981, total=   6.3s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    6.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................................... , score=0.985, total=   4.4s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   10.7s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................................... , score=0.986, total=   3.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   14.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9840749870948032"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply Logistic Regression on training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_reg = LogisticRegression(solver=\"liblinear\", class_weight='balanced', max_iter=100000)\n",
    "score = cross_val_score(log_reg, X_train_transformed, y_train, cv=3, verbose=3)\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.981, total=   0.4s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................................... , score=0.949, total=   0.4s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.8s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................................... , score=0.955, total=   0.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9617834394904459"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply on test set\n",
    "\n",
    "# Define vocabulary from both X_test and X_train\n",
    "voc_eH = emailHandler(X)\n",
    "vocabulary = voc_eH.make_vocabulary()\n",
    "\n",
    "# Define X_test vector\n",
    "test_eH = emailHandler(X_test) \n",
    "X_test_transformed = test_eH.create_vector(vocabulary) # Create a vector with general vocabulary\n",
    "cross_val_score(log_reg, X_test_transformed, y_test, cv=3, verbose=3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 99.26%\n",
      "Recall: 99.01%\n"
     ]
    }
   ],
   "source": [
    "# Check precision and recall score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "# Use general vocabulary\n",
    "X_train_transformed = eH.create_vector(vocabulary)\n",
    "\n",
    "log_reg.fit(X_train_transformed, y_train)\n",
    "y_pred = log_reg.predict(X_test_transformed)\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
